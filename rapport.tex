\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts}
\usepackage[left=15mm, right=15mm, top = 15mm]{geometry}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\DeclareMathOperator*{\argmax}{argmax}

\title{Kaggle Competition - Rapport}
\author{Guillaume Genois, 20248507}
\date{November 2024}

\begin{document}

\maketitle

\section{Introduction}
La présente compétition consiste à trouver $f \in F$ avec $F$ une famille riche de fonctions (linéaire, arbres de décisions, bayésiennes, etc.) pour un problème de classification binaire sur un corpus de texte qui est représenté par des vecteurs de compte. Les données sont déjà séparées pour le test et l'entraînement tel que, $X_{train} \in \mathbb{N}^{9000 \times 26000}$, et $X_{test} \in \mathbb{N}^{2300 \times 26000}$. Étant donné la sparsité des données, et le fait que les vecteurs d'entrées sont des fréquences, nous avons eu l'intuition de prioriser les approches fréquentistes qui sont robustes face au problème de sparsité, sont adaptés pour les données de fréquences, et sont relativement rapide à implémenter. Pour le premier jalon, nous avons implémenter un modèle Naif de Bayes avec validation croisée pour le lissage de Laplace, sans prétraitement des données. Cette méthode nous a donné sur un score de ... sur la validation et de 0.7358... sur l'ensemble de test fournis. Pour le deuxième jalon, nous avons fait un prétraitement plus exhaustif des données, en explorant les techniques pour réduire la dimensionnalité, balancer les données en sur-échantillonant ou en sous-échantillonant, retirer les mots sans importance, et appliquer des transformations TFIDF à nos ensembles d'entraînement et de test indépendamment. De plus, nous avons élargi l'ensemble de fonctions à tester allant de familles de modèles fréquentistes (tel que la Naive de Bayes et le Complement de Bayes), modèles à base d'arbres (XGBoost), modèles linéaires (SVM et régression logistique) et modèles ensemblistes (ensemble models en anglais) par vote. Nous avons également utilisé la procédure de validation croisée pour trouver les meilleurs hyperparamètres de chacun de ces modèles. Le modèle le plus performant était un modèle ensembliste qui avait un score F1 de 0.66 sur l'ensemble de validation et 0.74... sur l'ensemble de test.

\section{Conception de fonctionnalité}
Nous avons constater plusieurs axes de prétraietment qui pourraient améliorer la performance de nos algorithmes. Nous énumérons les techniques utilisées dans cette section, mais il est important de noter que ces méthodes ne sont pas utilisées ensemble 
\subsection{Rééchantillonage}
Comment mentionné, n'avons pas appliqué de prétraitement de données lors du premier jalon, nous avons directement appliqué la Naive de Bayes à nos données. Cela dit, pour le deuxième jalon, nous avons tout d'abord constaté le débalancement de classe dans $X_{train}$ étant d'environ $76\%$ dans la classe 1 et de $14\%$ dans la classe 0. Les sous-sections suivantes discutent nos choix de rééchantillonage.
\subsubsection{SMOTE}
Nous avons d'abord tenter de suréchantilloner notre jeu de données avec une technique de génération d'échantillon synthétique, soit SMOTE. Cette méthode se base l'algorithme de K plus proche voisins pour générer des nouveaux points de données dans le voisinage d'un point de la classe minoritaire. Pour un point donné $x_i$ et $x_{voisin}$ un des k points les plus proches de $x_i$, on génère $x_{synth}$ ainsi:\\
$$x_{synth} = x_i + \lambda (x_{voisin} - x_i)$$ avec $\lambda \in [0, 1]$.
Nous générons $1000$ observations dans la classe minoritaire.
\subsubsection{Boostrap}
Étant donnée que le ratio entre les données d'entraînement et test est de $20\%$, nous avons songé à l'impact de réduire le nombre d'observation de la classe majoritaire au point d'avoir une séparation entraînement/test d'environ 70/30, plutôt que 80/20. Nous obtenons la proportion 70/30 en retirant la moitié des données dans la classe majoritaire, aléatoirement. Nos classes restes débalancées, mais nous passons d'un ratio de 76/14 à environ 60/40. Voir annexes pour ces graphiques
\subsection{Réduction de dimensions}
Étant donné que nos vecteurs sont de très haute dimension, nous avons penser à retirer des attributs (qui sont des mots) pour faciliter l'entraînement et enlever des atributs qui n'apportent aucune information quant à la discrimination dans la classe 0 ou 1. Nous avons testé deux méthodes pour réduire les 
\subsubsection{Réduction à base d'arbres}
Nous utilisons un arbre de décision avec le critère de Gini pour évaluer l'importance des attributs. Le coefficcient de Gini est calculé ainsi: $\text{Gini} = 1 - \sum_{i=1}^{C} \mathbb{P}[\text{un points choisi aléatoirement appartient à $C_i$}]^2$ avec $C_i \in C$. À chaque division, l’arbre choisit l’attribut qui réduit le plus l'impureté de Gini, indiquant ainsi les attributs les plus discriminants pour la prédiction. En analysant l'importance cumulative de chaque attribut dans l'ensemble de l'arbre, nous extrayons les $k$ attributs les plus importants, ceux-ci étant les plus déterminants pour la prédiction.
\subsubsection{Réduction à base de fréquences cumulatives}
Sachant que le jeu de données est très éparse,  nous avons tenter de retirer les mots qui surviennent très rarement à travers le corpus de texte. Nous détectons ces mots en calculant la somme des fréquences d'un mot à travers tout le corpus, en triant le corpus en ordre croissant, et en retirant une proportion des mots du jeu de données en tronquant le vecteurs de fréquences cumulatives, à l'indice qui couvre la proportion voulue du corpus. En somme, nous retirons les mots qui ne sont pas fréquents dans le corpus Voir Annexe pour algorithme plus détaillé. 
\subsection{Transformation TFIDF}

\section{Premier jalon}
Pour l'implémentation du premier jalon, nous avons choisi d'implémenter un classifieur de Bayes Naif. Pour ce premier jalon, nous n'avons apporté de prétrairement aux données. Nous nous sommes uniquement concentré sur l'implementation du modèle, et de la validation croisée de celui-ci.

\subsection{Classifieur de Bayes Naif}
D'abord et avant tout, notre argument pour utiliser ce modèle était que nous avions des vecteurs de compte (et donc de fréquence) en haute dimension. La Naive de Bayes prend moins d'effort de régularisation contrairement à un modèle comme la régression logistique qui est plus prône d'être affecté par la haute dimensionnalité sans critère de régularisation bien défini.
\subsubsection{Entraînement}
Lors de la phase d'entraînement, estime le postérieur de Bayes par le jeu de donnés d'entraînement. Posons, $C \in \{0, 1\}$ étant la variable réponse $0$ ou $1$, et $x_i \in X_{train}$. Une hypothèse cruciale à cet algorithme est l'indépendance des attributs tel que $\forall x_i \in X \quad \forall j, k \in x_i$ avec $j$ et $k$ des composantes du vecteur $x_i$, nous avons:
$$\mathbb{P}[X = x_{i, j}, X = x_{i, k}] = \mathbb{P}[X = x_{i, j}] \mathbb{P}[X = x_{i, k}]$$ Nous avons par le théorème de Bayes:\\
$$\mathbb{P}[C | X=x_i] = \frac{\mathbb{P}[X = x_{i}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]} =  \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
Le critère de classification est le suivant:
$$\argmax_{C_i \in C, x_i \in X} \quad P[C_i | X = x_i] = \argmax_{C_i \in C, x_i \in X} \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
De façon équivalente, nous pouvons écrire le critère en fonction de la vraisemblance (le numérateur de l'expression ci-haut), car maximiser la probabilité, est équivalent à maximiser la vraisemblance, qui est aussi équivalent à maximiser la log vraisemblance, car les deux sont des fonctions croissantes. Voici le critère réécrit en fonction de la log vraisemblance:
$$\argmax_{C_i \in C, x_i \in X} \log{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]} = \argmax_{C_i \in C, x_i \in X} \sum_{j = 1}^{d} \log{\mathbb{P}[X = x_{i,j}| C]} + \log{\mathbb{P}[C]}$$
En pratique, il est important de voir que toutes les probabilités dans les expressions ci-haut sont calculables par le biais de fréquences observées dans le jeu de données d'entraînement. $\mathbb{P}[C]$ est la probabilité à priori d'être dans la classe 0 ou 1, donc $\mathbb{P}[C] = \frac{\text{Nombre d'observations dans la classe $i$}}{\text{Nombre d'observations dans $X_{train}$}}$. Similairement, $\mathbb{P}[X = x_{i, j}| C]$ est la probabilité que $x_{i, j}$, sachant la classe $C$, qui représente $\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$}}{\text{Nombre total de mots dans la classe $C_i$}}$. Cela dit, il est important de noter qu'il faut lisser cette probabilité afin d'éviter d'annuler la vraisemblance qui se produit lorsque $\mathbb{P}[X = x_{i, j}| C] = 0$, qui survient lorsque un mot $j$ ne se trouve pas dans la classe $C_i$. Posons $\alpha \in \mathbb{R}^{+}$ et $\alpha > 0$ l'hyperparamètre de lissage. Nous calculons dorénavant la vraisemblance, $$\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$ + $\alpha$}}{\text{Nombre total de mots dans la classe $C_i$ + $\alpha \times$ nombre de mots dans le vocabulaire}}$$. Une grande valeur pour $\alpha$ applique un fort lissage sur toute les probabilités, alors qu'une faible valeur de $\alpha$ produit un lissage plus dur. Nous pouvons trouver le $\alpha$ optimal par validation croisée. 
\subsection{Validation croisée}
Nous implémentons une validation croisée (k-fold cross validation) avec $k = 7$ pour trouver l'hyperparamètre $\alpha$ optimal. Nous avons établi un espace hyperparamétrique de valeur espacée uniformément, allant de $0.4$ à $1.05$ avec des sauts de $0.05$. Du au fait que l'espace est relativement restreint, cet espace est visité séquentiellement par une recherche en grille (grid-search). 


\section{Algorithmes d'apprentissage utilisés}
Nous avons effectué multiples expériences avec multiples algorithmes d'apprentissage différents. Les algorithmes qui ont été les plus efficaces sont ceux présentés ci-après.
\subsection{Classifieur Complément de Bayes naïf}

Comme nous avons décrit le complément de Bayes

\subsection{Classifieur XGBoost}
Nous avons également exploré l’application d’un modèle basé sur des arbres au problème. Cet algorithme crée une série d'arbres de décision, chaque arbre subséquent étant formé pour corriger les erreurs des arbres précédents, en utilisant les gradients et Hessians de l’erreur pour orienter ses subdivisions. Contrairement au vote majoritaire utilisé dans des méthodes comme la forêt aléatoire, XGBoost produit une prédiction finale en sommant les prédictions pondérées de chaque arbre. Comme pour le critère de Gini mentionné plus haut, XGBoost utilise un critère de gain de subdivision pour déterminer les branchements de chaque arbre de décision. Ce gain de subdivision est défini par la formule suivante : 
$$\text{Gain} = \frac{1}{2} \left( \frac{(\sum \text{gradients}_{\text{gauche}})^2}{\sum \text{Hessians}_{\text{gauche}} + \lambda} + \frac{(\sum \text{gradients}_{\text{droite}})^2}{\sum \text{Hessians}_{\text{droite}} + \lambda} - \frac{(\sum \text{gradients}_{\text{total}})^2}{\sum \text{Hessians}_{\text{total}} + \lambda} \right) - \gamma$$
Avec $\sum \text{gradients}$ étant la somme des gradients pour le sous-ensmeble à gauche ou droite de la subdivision, 
Avec $\sum \text{gradients}_{\text{total}}$ somme des gradients pour l’ensemble complet avant la subdivision. $\sum \text{Hessians}$ est la somme des Hessiens (dérivées secondes). Notons que les gardients et Hessiens sont calculés à partir de la perte d'entropie croisée: $$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i) \right)$$
Avec respectivement la dérivée première pour le gradient et seconde pour l'Hessien.
$$\frac{\partial \text{Log Loss}}{\partial \hat{y}_i} = \frac{\hat{y}_i - y_i}{\hat{y}_i (1 - \hat{y}_i)}$$
$$\frac{\partial^2 \text{Log Loss}}{\partial \hat{y}_i^2} = \frac{1 - y_i}{\hat{y}_i^2} + \frac{y_i}{(1 - \hat{y}_i)^2}
$$
Avec  $\hat{y}_i$ est la classe prédite et $y_i$ est la vraie classe.
XGBoost utilise ainsi la somme des gradients et des Hessians pour guider la construction de chaque arbre, en choisissant les divisions qui maximisent le gain de subdivision pour minimiser l’erreur totale.  
\subsection{Classifieur régression ridge}
\subsection{Classifieur d'apprentissage par ensembles}
Nous avons tenter combiner plusieurs modèles de nature différentes et aggréger leurs prédictions dans le but d'augmenter la performance sur le score F1. Cette procédure consiste à entraîner indépendamment des classificateurs et procéder à un vote lisse en extrayant et aggrégeant les probabilités de classification de chaque modèle dans l'ensemble. Dans notre cas, nous avons entraîner un classifieur complémentaire de Bayes (une modèle fréquentiste), un classifieur XGBoost (modèle à base d'arbres de décision), et une régression logistique (classifieur linéaire). Il est important de noter que nous avons choisi ces modèles pour avoir une frontière de décision de nature différente, et avoir des estimations plus robustes, qui ne sont pas biaisées par la similarité de la frontière de décision apprise entre les modèles. Voir algorithme 1 dans l'annexe, pour une description plus rigoureuse de la méthode.

\subsection{Autres}
Ces prochains algorithmes de classification ont aussi été essayés sans que leurs résultats soient assez concluants.
\subsubsection{Classifieur par régression logistique}
\subsubsection{Classifieur SVM}
\subsubsection{Classifieur par fôrets aléatoires}
\subsubsection{Classifieur par réseaux de neurones}

\section{Méthodologie}
\subsection{Répartition pour l'entrainement et la validation}
Pour tout les algorithmes et prétraitement choisis, nous avons appliquer une validation croisée k-fold avec $k = 5$ stratfiées. La stratification dans la séparation des données en ensemble d'entraînement et de validation est cruciale car nous avons un jeu de données débalancés (malgré le rebalancement mentionné plus haut). Ainsi, nous avons une représentation proportionelle des étiquettes de la classe $0$ et $1$ dans la l'ensemble d'entraînement et de validation. 
\subsection{Ajustement des hyperparamètres}
Nous utilisons la procédure de recherche aléatoire pour trouver la meilleure combinaison d'hyperparamètres. La meilleure combinaison d'hyperparamètres est choisies en moyennant le score F1 pour chaque combinaison d'hyperparamètres Nous priorisons cette procédure plutôt qu'une recherche en grille par soucis computationnel. 
\subsection{Astuces d'optimisation}

\section{Résultats}
\subsection{Étape 1}
\subsection{Étape 2}

\section{Discussion}
\subsection{Avantages et inconvénients}
\subsection{Améliorations futures}

\section{Références}
les sites desquels nous nous sommes inspirés

\section{Annexes}
\subsection{Graphiques}
\subsection{Algorithmes}
\subsubsection{Classifieur par ensemble}
\begin{algorithm}
\caption{Algorithme de Vote Lisse pour un Classifieur Ensembliste}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Un ensemble de modèles  $\Gamma = \{\gamma_1, \gamma_2, \dots, \gamma_k\}$, des poids associés $\{w_1, w_2, \dots, w_k\}$, un échantillon à classifier $x$.
\STATE \textbf{Output:} Classe prédite pour l'échantillon $x$.

\FORALL{$\gamma_i \in \Gamma$ }
    \STATE Obtenir la probabilité prédite pour chaque classe $c$ : $P(c|x, M_i)$.
\ENDFOR

\FOR{chaque classe $c$}
    \STATE Calculer la probabilité agrégée pour la classe $c$ :
    \[
    P(c|x) = \frac{\sum_{i=1}^k w_i \cdot P(c|x, M_i)}{\sum_{i=1}^k w_i}
    \]
\ENDFOR

\STATE Retourner la classe avec la probabilité agrégée maximale :
\[
\hat{y} = \arg\max_c P(c|x)
\]
\end{algorithmic}
\end{algorithm}
graphiques
\end{document}