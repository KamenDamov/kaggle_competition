\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts}
\usepackage[left=15mm, right=15mm, top = 15mm]{geometry}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} % For including graphics
\usepackage{svg}
\usepackage{algorithmic}
\DeclareMathOperator*{\argmax}{argmax}

\title{Compétition Kaggle - Rapport}
\author{Guillaume Genois, 20248507 \\Kamen Damov, 20102811}
\date{12 novembre 2024}

\begin{document}

\maketitle

\section{Introduction}
La présente compétition consiste à trouver $f \in F$ avec $F$ une famille riche de fonctions (linéaire, arbres de décisions, bayésiennes, etc.) pour un problème de classification binaire sur un corpus de texte qui est représenté par des vecteurs de compte. Les données sont déjà séparées pour le test et l'entraînement tel que, $X_{train} \in \mathbb{N}^{k \times d}$, et $X_{test} \in \mathbb{N}^{m \times d}$. Étant donné la sparsité des données, et le fait que les vecteurs d'entrées sont des fréquences, nous avons prioriser les approches bayésiennes qui sont robustes face au problème de sparsité, sont bien adaptées aux données de fréquences, et sont relativement rapides à implémenter. Pour le premier jalon, nous avons implémenter un modèle Naif de Bayes avec validation croisée pour le lissage de Laplace, sans prétraitement des données. Cette méthode nous a donné sur un score de 0.6 sur la validation et de 0.719  sur l'ensemble de test privés. Pour le deuxième jalon, nous avons fait un prétraitement plus exhaustif des données, en explorant les techniques pour réduire la dimensionnalité, balancer les données en sur-échantillonant ou en sous-échantillonant, retirer les mots sans importance, et appliquer des transformations TF-IDF à nos ensembles d'entraînement et de test indépendamment. De plus, nous avons élargi l'ensemble de fonctions à tester allant de familles de modèles bayésiens (tel que la Naïve de Bayes et le Complément de Bayes), modèles à base d'arbres (XGBoost), modèles linéaires (SVM et régression logistique) et modèles ensemblistes (ensemble models en anglais) par vote. Nous avons également utilisé la procédure de validation croisée pour trouver les meilleurs hyperparamètres de chacun de ces modèles. Le modèle le plus performant était un modèle ensembliste qui avait un score F1 de 0.68 sur l'ensemble de validation et 0.74 sur l'ensemble de test.

\section{Conception de fonctionnalité}
Nous avons constater plusieurs axes de prétraitement qui pourraient faciliter l'apprentissage de nos algorithmes. Nous énumérons les techniques utilisées dans cette section, mais il est important de noter que ces méthodes ne sont pas nécessairement utilisées ensembles. Comment mentionné, nous n'avons pas appliqué de prétraitement de données lors du premier jalon, nous avons directement appliqué la Naïve de Bayes à nos données. Cela dit, pour le deuxième jalon, nous avons tenté d'appliquer certaines méthodes mentionnées ci-après.
\subsection{Rééchantillonage}
Nous avons tout d'abord constaté le débalancement de classe dans $X_{train}$ était d'environ $76\%$ pour la classe 0 et de $14\%$ pour la classe 1. Les sous-sections suivantes discutent de nos choix de rééchantillonage.
\subsubsection{SMOTE}
Nous avons d'abord tenter de suréchantilloner notre jeu de données avec une technique de génération d'échantillon synthétique, soit SMOTE. Cette méthode se base sur l'algorithme de K plus proche voisins pour générer des nouveaux points de données dans le voisinage d'un point de la classe minoritaire. Pour un point donné $x_i$ et $x_{voisin}$ un des k points les plus proches de $x_i$, on génère $x_{synth}$ ainsi:\\
$$x_{synth} = x_i + \lambda (x_{voisin} - x_i)$$ avec $\lambda \in [0, 1]$.
Nous générons $1000$ observations dans la classe minoritaire.
\subsubsection{Sous-échantillonage}
Si nous tentons plutôt de sous-échantilloner, il nous est possible de retirer des observations aléatoirement dans la classe majoritaire jusqu'à un seuil choisi. Pour la compétition, nous avons choisi de retirer la moitié des données dans la classe majoritaire ce qui nous permet de garder nos classes débalancées, mais nous passons d'un ratio de 76/14 à environ 60/40. Par ce fait même, nous retirons environ 3000 lignes du jeu d'entraînement, mais nous considérons qu'une taille d'environ 7000 lignes reste raisonnable pour l'entrainement. Voir annexes pour ces graphiques (Figure 3 à 6).
\subsection{Réduction de dimensions}
Étant donné que nos vecteurs sont de très hautes dimensions, nous avons penser à retirer des attributs (qui sont les mots inclus dans le corpus de texte fournis) pour accélérer l'entraînement et enlever les attributs qui n'apportent aucune information quant à la discrimination dans la classe 0 ou 1. Nous avons testé deux méthodes pour réduire les dimensions selon un calcul. Deux autres méthodes ont aussi été essayées en prenant plutôt en compte le sens des mots.
\subsubsection{Réduction à base d'arbres}
Nous utilisons un arbre de décision avec le critère de Gini pour évaluer l'importance des attributs. Le coefficcient de Gini est calculé ainsi: $\text{Gini} = 1 - \sum_{i=1}^{C} \mathbb{P}[\text{un points choisi aléatoirement appartient à $C_i$}]^2$ avec $C_i \in C$. À chaque division, l’arbre choisit l’attribut qui réduit le plus l'impureté de Gini, indiquant ainsi les attributs les plus discriminants pour la prédiction. En analysant l'importance cumulative de chaque attribut dans l'ensemble de l'arbre, nous extrayons les $k$ attributs les plus importants, ceux-ci étant les plus déterminants pour la prédiction.
\subsubsection{Réduction à base de fréquences cumulatives}
Sachant que le jeu de données est très éparse,  nous avons tenter de retirer les mots qui surviennent très rarement à travers le corpus de texte. Nous détectons ces mots en calculant la somme des fréquences d'un mot à travers tout le corpus, en triant le corpus en ordre croissant, et en retirant une proportion des mots du jeu de données en tronquant le vecteurs de fréquences cumulatives, à l'indice qui couvre la proportion voulue du corpus. En somme, nous retirons les mots qui ne sont pas fréquents dans le corpus. Voir Annexe pour algorithme plus détaillé. 
\subsubsection{Réduction en retirant les mots vides}
En langage naturel, certains mots n'apportent pas de sens à une phrase. Ils ne sont présents que pour faire les liaisons et les transitions fluides entre les mots. Ceux-ci se font appeler les mots vides ("stopwords"). En anglais, des exemples de ces mots sont "the", "of", "a", etc. En dessinant le graphique de la fréquence de ces mots, il est possible de remarquer que certains apparaissent très souvents tels que juste qu'à $10 000$ fois à travers les documents. Il apparait alors intéressant de retirer les colonnes de ces mots dans nos jeux d'entrainement et de test. La dimensionnalité diminue alors d'environ ... colonnes.
\subsubsection{Réduction en combinant les mots similaires}
Similairement, des librairies telles que nltk offrent des méthodes qui transforment les mots vers leur mot commun (lemmatization) ou encore vers leur racine (stemming). Ensuite, toutes les colonnes dont leur mot est le même, il est possible de les combiner ensemble en additionnant leurs fréquences. La dimensionnalité diminue alors d'environ ... colonnes.
\subsection{Transformation TF-IDF}
Afin de mesurer l'importance d'un mot selon sa fréquence à travers les documents, il est possible de le transformer avec TF-IDF. La formule est la suivante :
\[m_{x,y} = {tf}_{x,y} * log(\frac{n}{1 + df_x})\]
où $tf_{x,y}$ est la fréquence du mot x dans le document y, $n$ est le nombre de documents et $df_x$ est le nombre de documents dans lesquels le mot x apparait.\\
Ainsi, les mots apparaissant souvent à travers tous les documents auront un moins gros poids ce qui devrait faire diminuer leur bruit.

\section{Algorithmes utilisés}
Nous avons effectué multiples expériences avec multiples algorithmes d'apprentissage différents. Les algorithmes qui ont été les plus efficaces sont ceux présentés ci-après.

Pour l'implémentation du premier jalon, nous avons choisi d'implémenter un classifieur de Bayes Naif. Pour ce premier jalon, nous n'avons apporté de prétrairement aux données. Nous nous sommes uniquement concentré sur l'implementation du modèle, et de la validation croisée de celui-ci.

\subsection{Classifieur de Bayes Naif}
D'abord et avant tout, notre argument pour utiliser ce modèle était que nous avions des vecteurs de compte (et donc de fréquence) en haute dimension. La Naïve de Bayes prend moins d'effort de régularisation contrairement à un modèle comme la régression logistique qui est plus prône d'être affecté par la haute dimensionnalité sans critère de régularisation bien défini.
Lors de la phase d'entraînement, on estime le postérieur de Bayes (probabilités conditionnelles) par le jeu de donnés d'entraînement. Posons, $C \in \{0, 1\}$ étant la variable réponse $0$ ou $1$, et $x_i \in X_{train}$. Une hypothèse cruciale à cet algorithme est l'indépendance des attributs (ou des mots dans le présent problème). Nous avons donc par le théorème de Bayes:\\
$$\mathbb{P}[C | X=x_i] = \frac{\mathbb{P}[X = x_{i}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]} =  \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
Le critère de classification est le suivant:
$$\argmax_{C_i \in C} \quad P[C_i | X = x_i] = \argmax_{C_i \in C} \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
De façon équivalente, nous pouvons écrire le critère en fonction de la vraisemblance (le numérateur de l'expression ci-haut), car maximiser la probabilité, est équivalent à maximiser la vraisemblance, qui est aussi équivalent à maximiser la log vraisemblance, car les deux sont des fonctions croissantes. Voici le critère réécrit en fonction de la log vraisemblance:
$$\argmax_{C_i \in C} \log{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]} = \argmax_{C_i \in C} \sum_{j = 1}^{d} \log{\mathbb{P}[X = x_{i,j}| C]} + \log{\mathbb{P}[C]}$$
En pratique, il est important de voir que toutes les probabilités dans les expressions ci-haut sont calculables par le biais de fréquences observées dans le jeu de données d'entraînement. $\mathbb{P}[C]$ est la probabilité à priori d'être dans la classe 0 ou 1, donc $\mathbb{P}[C] = \frac{\text{Nombre d'observations dans la classe $i$}}{\text{Nombre d'observations dans $X_{train}$}}$. Similairement, $\mathbb{P}[X = x_{i, j}| C]$ est la probabilité que $x_{i, j}$, sachant la classe $C$, qui représente $\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$}}{\text{Nombre total de mots dans la classe $C_i$}}$. Cela dit, il est important de noter qu'il faut lisser cette probabilité afin d'éviter d'annuler la vraisemblance qui se produit lorsque $\mathbb{P}[X = x_{i, j}| C] = 0$, qui survient lorsque un mot $j$ ne se trouve pas dans la classe $C_i$. Posons $\alpha \in \mathbb{R}^{+}$ et $\alpha > 0$ l'hyperparamètre de lissage. Nous calculons dorénavant la vraisemblance, $$\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$ + $\alpha$}}{\text{Nombre total de mots dans la classe $C_i$ + $\alpha \times$ nombre de mots dans le vocabulaire}}$$. Une grande valeur pour $\alpha$ applique un fort lissage sur toute les probabilités, alors qu'une faible valeur de $\alpha$ produit un lissage plus dur. Nous pouvons trouver le $\alpha$ optimal par validation croisée. 
\subsection{Classifieur Complément de Bayes naïf}
Contrairement au classifieur Naïve Bayes décrit précédemment, le Classifieur Complément de Bayes calcule les statistiques des caractéristiques (comme les fréquences des mots) en fonction des classes autres que celle que l’on cherche à prédire. C'est pour cela qu'on parle de "complément" : pour chaque classe c, on prend en compte les informations des classes complémentaires, ce qui permet de compenser les erreurs d’estimation dans les classes minoritaires. Cette approche tend à réduire l’impact des caractéristiques qui sont très spécifiques à certaines classes et qui pourraient biaiser le modèle. Pour ce qui est des hyperparamètres de ce modèle, c'est le même $\alpha$ que dans la Naive de Bayes vanille expliqué ci-haut.
\subsection{Classifieur XGBoost}
Nous avons également exploré l’application d’un modèle basé sur des arbres au problème. Cet algorithme crée une série d'arbres de décision, chaque arbre subséquent étant formé pour corriger les erreurs des arbres précédents, en utilisant les gradients et Hessians de l’erreur pour orienter ses subdivisions. Ces gradients et Hessians sont calculé par la derivée seconde et première de la fonction de perte d'entropie croisée. Voir annexe pour plus de détails sur les dérivations mathématiques. Contrairement au vote majoritaire utilisé dans des méthodes comme la forêt aléatoire, XGBoost produit une prédiction finale en sommant les prédictions pondérées de chaque arbre. Comme pour le critère de Gini mentionné plus haut, XGBoost utilise un critère de gain de subdivision pour déterminer les branchements de chaque arbre de décision. 

XGBoost utilise ainsi la somme des gradients et des Hessians pour guider la construction de chaque arbre, en choisissant les divisions qui maximisent le gain de subdivision pour minimiser l’erreur totale.  

\subsection{Classifieur perte de Huber modifié avec descente de gradient stochastique (SGD)}
Le Classifieur SGD (Stochastic Gradient Descent) avec la perte Modified Huber est une méthode de classification linéaire qui utilise la descente de gradient stochastique pour minimiser une fonction de coût basée sur la distance entre les prédictions et les étiquettes cibles. La fonction de perte Huber modifiée est une variante de la perte Hinge, utilisée dans les SVMs, mais avec une modification qui la rend lisse et plus tolérante aux valeurs aberrantes (outliers). L'algorithme SGD utilise un échantillonnage aléatoire des données pour mettre à jour les paramètres du SVM à chaque itération, au lieu d'utiliser l’ensemble des données d'apprentissage, ce qui le rend rapide et efficace, même pour des jeux de données volumineux.

\subsection{Classifieur SVM (SVC)}
Le principe d'un SVM est de trouver un hyperplan qui sépare les données de différentes classes avec la plus grande marge possible, c'est-à-dire la distance maximale entre les points de chaque classe les plus proches de l'hyperplan. Une fois l'hyperplan trouvé, le SVM classe un nouvel exemple en fonction de son côté par rapport à l'hyperplan : si le point est d’un côté, il appartient à la classe positive, sinon il appartient à la classe négative.

\subsection{Classifieur par régression logistique}
La régression logistique utilise une fonction sigmoïde pour transformer la sortie linéaire en une probabilité comprise entre 0 et 1, ce qui permet de modéliser la probabilité d'appartenance à une classe. Le modèle cherche à optimiser les coefficients associés à chaque caractéristique pour minimiser la différence entre les prédictions et les étiquettes réelles. Si la probabilité est assez haute d'être dans une classe, cette classe lui est attribuée.

\subsection{Classifieur d'apprentissage par ensembles}
Nous avons tenter de combiner plusieurs modèles de nature différentes et d'aggréger leurs prédictions dans le but d'augmenter la performance sur le score F1. Cette procédure consiste à entraîner indépendamment des classifieurs et procéder à un vote lisse en extrayant et aggrégeant les probabilités de classification de chaque modèle dans l'ensemble. Dans notre cas, nous avons entraîner un classifieur complémentaire de Bayes (modèle fréquentiste), un classifieur XGBoost (modèle à base d'arbres de décision), et un classifieur SVM sous perte Huber modifiée à descente de gradient stochastique (modèle linéaire). Il est important de noter que nous avons choisi ces modèles pour avoir une frontière de décision de nature différente, et avoir des estimations plus robustes, qui ne sont pas biaisées par la similarité de la frontière de décision apprise entre les modèles. Voir algorithme 1 dans l'annexe, pour une description plus rigoureuse de la méthode.

\section{Méthodologie}
\subsection{Répartition pour l'entrainement et la validation}
Pour tout les algorithmes et prétraitement choisis, nous avons appliqué une validation croisée k-fold avec $k = 5$ stratifiées. La stratification dans la séparation des données en ensemble d'entraînement et de validation est cruciale, car nous avons un jeu de données débalancés (malgré le rebalancement mentionné plus haut). Ainsi, nous avons une représentation proportionelle des étiquettes de la classe $0$ et $1$ dans l'ensemble d'entraînement et de validation.

\subsection{Ajustement des hyperparamètres}
Nous utilisons la procédure de recherche aléatoire pour trouver la meilleure combinaison d'hyperparamètres. Celle-ci est choisie en prenant la combinaison ayant la meilleure moyenne du score F1 à travers les 5 divisions. Cette méthode s'applique à tout les algorithmes du deuxième jalon, par soucis computationnel et de production de résultats rapides. Pour le modèle bayésien du premier jalon, nous appliquons une recherche en grille, vu que nous avons qu'un seul hyperparamètre à ajuster. Comme mentionné ci-haut, autre que le modèle XGBoost, nous avons opté pour un espace restreint d'hyperparamètres, en ajustant prioritairement les poids du terme de régularisation. 
Pour le modèle bayésien du premier jalon, nous avions uniquement un hyperparamètre à explorer soit $\alpha$. Nous implémentons une validation croisée (k-fold cross validation) avec $k = 7$ pour trouver l'hyperparamètre $\alpha$ optimal. Nous avons établi un espace hyperparamétrique de valeur espacée uniformément, allant de $0.4$ à $1.05$ avec des sauts de $0.05$. Dû au fait que l'espace est relativement restreint, cet espace est visité séquentiellement par une recherche en grille (grid-search). Pour le modèle SVC, nous avons le coefficient du noyau RBF, et le poids attribué au terme de régularisation de la pénalité. Pour le classifieur SGD, nous avons utilisé le terme de régularisation ElasticNet, qui est une combinaison de terme de pénalité $l1$ et $l2$. Il s'en suit naturellement, que la proportion de chaque perte dans la régularisation, $l1_{ratio}$ est un hyperparamètre, que nous avons borné dans un espace de valeur linéairement espacée dans $[0.001, 1]$. Le poids de cette régularisation ElasticNet est modulée par un hyperparamètre qu'on trouve également par validation croisée qui prend des valeurs dans un espace séparé logarithimquement entre $[0.01, 1]$. Pour XGBoost, nous avons décider de restreindre l'espace hyperparamétrique à quatres hyperparamètres cruciaux. Le premier est le taux d'apprentissage qui est la taille du pas lors de la descente de gradient sur la perte d'entropie croisée pour calculer le gain de subdivision $\gamma \in \mathbb{R}$, $\gamma > 0$. Celui-ci prend des valeurs uniformément distribué entre $[0.01, 0.2]$. Le deuxième hyperparamètre est le nombre d'arbres que nous créons séquentiellement, prenant des valeurs de espacées de $100$ entre $[200, 500]$. Ensuite, nous avons la profondeur maximale de chaque arbres prenant des valeurs dans $[3, 5, 7, 10]$. Et finalement, nous avons un hyperparamètre pour la proportion de données à utiliser pour produire un arbre, ainsi contrôlant le sur-apprentissage. Cet hyperparamètre prend des valeurs uniformément distribuées entre $[0.4, 0.6]$.

\subsection{Astuces d'optimisation}
Nous avons opté pour une recherche aléatoire plutôt qu'une recherche en grille qui est plus chronophage pour un résultat comparable \cite{bergstra2012random}. Une autre optimisation a été de convertir les données de train et de test en int8. Ceci a permis de contourner plusieurs erreurs de mémoire et a permis d'accélérer le temps computationnel tout en conservant l'étendu de toutes les valeurs présentes dans les jeux de données. En effet, la fréquence maximale dans les matrices est de 38 ce qui est inférieure à $2^8=256$. Cependant, l'algorithme du SVC est une exception, car il a besoin d'avoir les données encodées en float32 ou float64 afin de faire ses calculs. Nous avons donc simplement rechanger les types.

\section{Résultats}
Par soucis de présentation, nous renommons le modèle Complément Naive de Bayes CNB, XGBoost par XGB, Régression Logistique par LogReg. Les modèles ensemblistes figurent sur la même ligne.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{SMOTE} & \textbf{Under sampl.} & \textbf{Tree} &
        \textbf{Cum Sum} & \textbf{Stopwords} & \textbf{TF-IDF} &
        \textbf{F1 Val} & 
        \textbf{F1 Test} \\
        \hline
        NB vanille &  &  &  &  &  &  & 0.5893 & 0.7196\\
        CNB &  &  &  &  &  & x & 0.5611 & 0.7127\\
        CNB &  &  & x &  & x & x & 0.5636 & 0.6324\\
        CNB & x &  & x &  & x & x & 0.6556 & 0.6870\\
        CNB &  & x &  & x & x &  & 0.7041 & N/A\\
        LogReg &  & x &  & x & x &  & 0.6308 & N/A\\
        SVM &  & x &  & x & x &  & 0.6415 & N/A\\
        SGD avec Huber mod. &  & x &  & x & x &  & 0.6294 & 0.6629\\
        XGBoost &  & x &  & x & x &  & 0.6592 & 0.6115\\
        CNB, XGB, LogReg &  & x &  & x & x &  & 0.5983 & 0.7233\\
        CNB, XGB, LogReg &  & x &  &  & x &  & 0.5956 & 0.7164\\
        CNB, XGB, SVC, SGD &  & x &  & x & x &  & 0.6895 & 0.7194\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et modèles}
    \label{tab:model_comparison}
\end{table}
En analysant le score F1, on peut voir une augmentation à première vue significative des méthodes d'apprentissages par ensemble contrairement aux performances de modèles uniques sur l'ensemble de test privé. Étant donné l'entraînement indépendants, de chaque modèle les valeurs des hyperparamètres dans les ensembles de modèles, sont les mêmes que les hyperparamètres optimaux trouvés dans les modèles individuels. Les modèles de bayésiens priorisent des valeurs arbitrairement petite ou grande de $\alpha$ ce qui nous indique que le lissage des probabilités dépend fortement des subdivisions aléatoires de validation pour la recherche du $\alpha$ optimal. Pour ce qui est des modèles à base d'arbres, ceux-ci priorisent plus d'estimateurs profonds, retournant les bornes maximales pour les hyperparamètres du nombre d'arbre $=500$, et profondeur maximale des arbres $=10$. Pour ce qui est des modèles linéaires (SGD, SVC, régression logistique), on peut observer un biais systématique pour un plus gros poids des termes de régularisations qui témoigne d'une préférence pour pour les modèles plus simples. Voir l'annexe pour des détails sur les valeurs exactes des hyperparamètres en lien avec la performance des modèles. Il est intéressant de noter que malgré la performance souvent plus faible sur l'ensemble de validation, les modèles ensemblistes produisent des résultats moins variables, quel que soit le choix de prétraitement de données, et modèles choisis. Ceci témoigne du lissage des erreurs de prédictions des modèles individuels ce qui diminue la variance des prédictions.
\section{Discussion}
Parler de pourquoi pas PCA, truncated-SVD ou autre?
\subsection{Avantages et inconvénients}
Un inconvénient de notre méthodologie a été que nous nous sommes lancés sur trop de modèles différents, alors qu'il aurait été plus efficace de se concentrer sur une poignée tels que seulement ceux décrits dans le rapport. La même méthodologie aurait dû être appliquée pour identifier clairement quelles méthodes de prétraitement étaient les meilleures.

Un avantage de notre méthodologie est la robustesse du classifieur par ensemble, car ils ont une plus petite variance que les modèles individuels. Un autre avantage est le travail sur la visualisation des données qui a permis une meilleure compréhension du matériel. Un dernier avantage est la structure du code en orienté objet qui a permis une plus grande facilité à adapter le code rapidement pour tester différentes méthodes de prétraitement (ce qui en a constitué un inconvénient aussi).

\subsection{Améliorations futures}
Une amélioration future à nos résultats serait de faire rouler notre modèle d'apprentissage par ensemble sur plus d'itérations de recherches d'hyperparamètres. En effet, celui-ci semble pouvoir converger vers un meilleure score F1 plus il y a d'itérations. Une autre amélioration possible serait d'identifier plus rigoureusement et exhaustivement quelles méthodes de prétraitement des données sont les plus efficaces au lieu de trop s'éparpiller sur plusieurs différentes.

\newpage
\section{Références}
\begin{enumerate}
    \item \label{bergstra2012random} Bergstra, J., \& Bengio, Y. (2012). Random search for hyper-parameter optimization. \textit{The Journal of Machine Learning Research, 13}, 281–305.
    \item Brownlee, J. (2020, August 20). \textit{Voting ensembles with Python}. Machine Learning Mastery. Tiré de \\    https://machinelearningmastery.com/voting-ensembles-with-python/
    \item Scikit-Learn developers. (2023). \textit{Document classification using 20 newsgroups}. Scikit-Learn. Tiré de \\ https://scikit-learn.org/1.5/auto\_examples/text/plot\_document\_classification\_20newsgroups.html

\end{enumerate}

\section{Annexes}
\subsection{Graphiques}
\begin{figure}[H]
    \centering
    % First image
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Cumulative Word Frequency Distribution with Stopwords Removed}.svg}
        \caption{Somme cumulative de fréquences des mots}
    \end{minipage}%
    \hfill
    % Second image
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Cumulative Word Frequency Distribution with Words Removed by Cumulative Sum}.svg}
        \caption{Somme cumulative de fréquences des mots - avec mots vide retirés}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    % First row
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Documents Length Frequency Distribution with Docs Removed by Undersampling}.svg}
        \caption{Fréquence de longueur des documents après sous échantillonnage}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Documents Length Frequency Distribution with No Transformation}.svg}
        \caption{Fréquence de longueur des documents sans transformation}
    \end{minipage}

    \vspace{0.5cm} % Space between rows

    % Second row
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Documents Length Frequency Distribution with Stopwords Removed}.svg}
        \caption{Fréquence de longueur des documents avec mots vides retirés}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Documents Length Frequency Distribution with Words Removed by Cumulative Sum}.svg}
        \caption{Fréquence de longueur des documents avec fréquences relatives retirées}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    % First row
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Overall Word Frequency Distribution with Docs Removed by Undersampling}.svg}
        \caption{Distributions de fréquences globales des mots avec sous-échantillonnage}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Overall Word Frequency Distribution with No Transformation}.svg}
        \caption{Distributions de fréquences globales des mots sans transformation}
    \end{minipage}

    \vspace{0.5cm} % Space between rows

    % Second row
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Overall Word Frequency Distribution with Stopwords Removed}.svg}
        \caption{Distributions de fréquences globales des mots avec mots vides retirés}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{visuals/{Overall Word Frequency Distribution with Words Removed by Cumulative Sum}.svg}
        \caption{Distributions de fréquences globales des mots avec somme cumulative retirée}
    \end{minipage}
\end{figure}


\subsection{Tables}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Model} & \textbf{$\alpha$} &
        \textbf{F1 Val} &
        \textbf{F1 Test}\\
        \hline
        Bayes naïf vanille & 0.45 & 0.5893 & 0.7196\\
        CNB & 0.0872 & 0.5611 & 0.7127\\
        CNB & 1.0980 & 0.5636 & 0.6324\\
        CNB & 0.0872 & 0.6556 & 0.6870\\
        CNB & 1.4261 & 0.7041 & N/A\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et valeurs d'hyperparamètres pour Les modèles bayésien}
    \label{tab:model_comparison}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Coefficient RBF} & \textbf{Poids régularisation} &
        \textbf{F1 Val} &
        \textbf{F1 Test}
        \\
        \hline
        SVM & 0.005 & 53 & 0.6415 & N/A\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et valeurs d'hyperparamètres pour SVM}
    \label{tab:model_comparison}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{l1 ratio} & \textbf{Poids régularisation} & \textbf{F1 Val} &
        \textbf{F1 Test}\\
        \hline
        SGD avec Huber modifié & 0.223 & 0.01 & 0.6294 & N/A\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et valeurs d'hyperparamètres pour SVM}
    \label{tab:model_comparison}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Learning rate} & \textbf{Nombre d'arbres} &
        \textbf{Profondeur maximale} &
        \textbf{subsample} &
        \textbf{F1 Val} &
        \textbf{F1 Test} \\
        \hline
        XGBoost & 0.0378 & 500 & 10 & 0.7465 & 0.6592 & 0.6629\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et valeurs d'hyperparamètres pour XGBoost}
    \label{tab:model_comparison}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Poids régularisation} &
        \textbf{Régularisation} &
        \textbf{Solveur} &
        \textbf{F1 Val} & 
        \textbf{F1 Test}\\
        \hline
        LogReg & 0.6808 & l1 & liblinear & 0.6308 & N/A\\
        \hline
    \end{tabular}
    \caption{Comparaison entre prétraitement et valeurs d'hyperparamètres pour régression logistique utilisée dans les modèles ensemblistes}
    \label{tab:model_comparison}
\end{table}
\subsection{Algorithmes et fondements mathématiques}
\subsubsection{XGBoost, perte entropie croisée, et gain de subdivision}
Le gain de subdivision (critère pour produire les branchements de l'arbre de décision) est défini par la formule suivante : 
 $$\text{Gain} = \frac{1}{2} \left( \frac{(\sum \text{gradients}_{\text{gauche}})^2}{\sum \text{Hessians}_{\text{gauche}} + \lambda} + \frac{(\sum \text{gradients}_{\text{droite}})^2}{\sum \text{Hessians}_{\text{droite}} + \lambda} - \frac{(\sum \text{gradients}_{\text{total}})^2}{\sum \text{Hessians}_{\text{total}} + \lambda} \right) - \gamma$$
 Avec $\sum \text{gradients}$ étant la somme des gradients pour le sous-ensmeble à gauche ou droite de la subdivision, 
 Avec $\sum \text{gradients}_{\text{total}}$ somme des gradients pour l’ensemble complet avant la subdivision. $\sum \text{Hessians}$ est la somme des Hessiens (dérivées secondes). Notons que les gardients et Hessiens sont calculés à partir de la perte d'entropie croisée: $$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i) \right)$$
Avec respectivement la dérivée première pour le gradient et seconde pour l'Hessien.
 $$\frac{\partial \text{Log Loss}}{\partial \hat{y}_i} = \frac{\hat{y}_i - y_i}{\hat{y}_i (1 - \hat{y}_i)}$$
 $$\frac{\partial^2 \text{Log Loss}}{\partial \hat{y}_i^2} = \frac{1 - y_i}{\hat{y}_i^2} + \frac{y_i}{(1 - \hat{y}_i)^2}
 $$
 Avec  $\hat{y}_i$ est la classe prédite et $y_i$ est la vraie classe.

\begin{algorithm}
\caption{Calcul de la fréquence cumulative et filtrage des mots les moins fréquents}
\begin{algorithmic}[1]
\Require Liste des mots $words$ et leur fréquence $freqs$, seuil de fréquence cumulative $threshold$
\Ensure Liste des mots filtrés $filtered\_words$

\State Trier $words$ par ordre décroissant de $freqs$
\State $cumulative\_sum \gets 0$
\State $filtered\_words \gets []$

\For{$i \gets 1$ to $\text{length}(words)$}
    \State $cumulative\_sum \gets cumulative\_sum + freqs[i]$
    \If{$cumulative\_sum \geq threshold$}
        \State \textbf{break}
    \Else
        \State Ajouter $words[i]$ à $filtered\_words$
    \EndIf
\EndFor

\State \Return $filtered\_words$

\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Algorithme de Vote Lisse pour un Classifieur Ensembliste}
\begin{algorithmic}[1]
\State \textbf{Input:} Un ensemble de modèles  $\Gamma = \{\gamma_1, \gamma_2, \dots, \gamma_k\}$, des poids associés $\{w_1, w_2, \dots, w_k\}$, un échantillon à classifier $x$.
\State \textbf{Output:} Classe prédite pour l'échantillon $x$.

\ForAll{$\gamma_i \in \Gamma$ }
    \State Obtenir la probabilité prédite pour chaque classe $c$: $P(c|x, M_i)$.
\EndFor

\For{chaque classe $c$}
    \State Calculer la probabilité agrégée pour la classe $c$ :
    \[
    P(c|x) = \frac{\sum_{i=1}^k w_i \cdot P(c|x, M_i)}{\sum_{i=1}^k w_i}
    \]
\EndFor

\State Retourner la classe avec la probabilité agrégée maximale :
\[
\hat{y} = \arg\max_c P(c|x)
\]

\end{algorithmic}
\end{algorithm}
\end{document}
