\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts}
\usepackage[left=20mm, right=20mm]{geometry}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\DeclareMathOperator*{\argmax}{argmax}

\title{Kaggle Competition - Rapport}
\author{kamendamov123 }
\author{Guillaume Genois, 20248507}
\date{November 2024}

\begin{document}

\maketitle

\section{Introduction}
La présente compétition consiste à trouver $f \in F$ avec $F$ une famille riche de fonctions (linéaire, arbres de décisions, bayésiennes, etc.) pour un problème de classification binaire sur un corpus de texte qui est représenté par des vecteurs de compte. Les données sont déjà séparées pour le test et l'entraînement tel que, $X_{train} \in \mathbb{N}^{9000 \times 26000}$, et $X_{test} \in \mathbb{N}^{2300 \times 26000}$. Étant donné la sparsité des données, et le fait que les vecteurs d'entrées sont des fréquences, nous avons eu l'intuition de prioriser les approches fréquentistes qui sont robustes face au problème de sparsité, sont adaptés pour les données de fréquences, et sont relativement rapide à implémenter. Pour le premier jalon, nous avons implémenter un modèle Naif de Bayes avec validation croisée pour le lissage de Laplace, sans prétraitement des données. Cette méthode nous a donné sur un score de ... sur la validation et de 0.7358... sur l'ensemble de test fournis. Pour le deuxième jalon, nous avons fait un prétraitement plus exhaustif des données, en explorant les techniques pour réduire la dimensionnalité, balancer les données en sur-échantillonant ou en sous-échantillonant, retirer les mots sans importance, et appliquer des transformations TFIDF à nos ensembles d'entraînement et de test indépendamment. De plus, nous avons élargi l'ensemble de fonctions à tester allant de familles de modèles fréquentistes (tel que la Naive de Bayes et le Complement de Bayes), modèles à base d'arbres (XGBoost), modèles linéaires (SVM et régression logistique) et modèles ensemblistes (ensemble models en anglais) par vote. Nous avons également utilisé la procédure de validation croisée pour trouver les meilleurs hyperparamètres de chacun de ces modèles. Le modèle le plus performant était un modèle ensembliste qui avait un score F1 de 0.66 sur l'ensemble de validation et 0.74... sur l'ensemble de test.

\section{Conception de fonctionnalité}
Nous avons constater plusieurs axes de prétraietment qui pourraient améliorer la performance de nos algorithmes.
\subsection{Rééchantillonage}
Comment mentionné, n'avons pas appliqué de prétraitement de données lors du premier jalon, nous avons directement appliqué la Naive de Bayes à nos données. Cela dit, pour le deuxième jalon, nous avons tout d'abord constaté le débalancement de classe dans $X_{train}$ étant d'environ $76\%$ dans la classe 1 et de $14\%$ dans la classe 0. Les sous-sections suivantes discutent nos choix de rééchantillonage.
\subsubsection{SMOTE}
Nous avons d'abord tenter de suréchantilloner notre jeu de données avec une technique de génération d'échantillon synthétique, soit SMOTE. Cette méthode se base l'algorithme de K plus proche voisins pour générer des nouveaux points de données dans le voisinage d'un point de la classe minoritaire. Pour un point donné $x_i$ et $x_{voisin}$ un des k points les plus proches de $x_i$, on génère $x_{synth}$ ainsi:\\
$$x_{synth} = x_i + \lambda (x_{voisin} - x_i)$$ avec $\lambda \in [0, 1]$.
Nous générons $1000$ observations dans la classe minoritaire.
\subsubsection{Boostrap}
Étant donnée que le ratio entre les données d'entraînement et test est de $20\%$, nous avons songé à l'impact de réduire le nombre d'observation de la classe majoritaire au point d'avoir une séparation entraînement/test d'environ 70/30, plutôt que 80/20. Nous obtenons la proportion 70/30 en retirant la moitié des données dans la classe majoritaire, aléatoirement. Nos classes restes débalancées, mais nous passons d'un ratio de 76/14 à environ 60/40. Voir annexes pour ces graphiques
\subsection{Réduction de dimensions}
Étant donné que nos vecteurs sont de très haute dimension, nous avons penser à retirer des attributs (qui sont des mots) pour faciliter l'entraînement et enlever des atributs qui n'apportent aucune information quant à la discrimination dans la classe 0 ou 1. Nous avons testé deux méthodes pour réduire les 
\subsubsection{Réduction à base d'arbres}
Nous utilisons un arbre de décision avec le critère de Gini pour évaluer l'importance des attributs. Le coefficcient de Gini est calculé ainsi: $\text{Gini} = 1 - \sum_{i=1}^{C} \mathbb{P}[\text{un points choisi aléatoirement appartient à $C_i$}]^2$ avec $C_i \in C$. À chaque division, l’arbre choisit l’attribut qui réduit le plus l'impureté de Gini, indiquant ainsi les attributs les plus discriminants pour la prédiction. En analysant l'importance cumulative de chaque attribut dans l'ensemble de l'arbre, nous extrayons les $k$ attributs les plus importants, ceux-ci étant les plus déterminants pour la prédiction.
\subsubsection{Réduction à base de fréquences cumulatives}
Sachant que le jeu de données est très éparse,  nous avons tenter de retirer les mots qui surviennent très rarement à travers le corpus de texte. Nous détectons ces mots en calculant la somme des fréquences d'un mot à travers tout le corpus, en triant le corpus en ordre croissant, et en retirant une proportion des mots du jeu de données en tronquant le vecteurs de fréquences cumulatives, à l'indice qui couvre la proportion voulue du corpus. En somme, nous retirons les mots qui ne sont pas fréquents dans le corpus Voir Annexe pour algorithme plus détaillé. 
\subsection{Transformation TFIDF}

\section{Premier jalon}
Pour l'implémentation du premier jalon, nous avons choisi d'implémenter un classifieur de Bayes Naif. Pour ce premier jalon, nous n'avons apporté de prétrairement aux données. Nous nous sommes uniquement concentré sur l'implementation du modèle, et de la validation croisée de celui-ci.

\subsection{Classifieur de Bayes Naif}
D'abord et avant tout, notre argument pour utiliser ce modèle était que nous avions des vecteurs de compte (et donc de fréquence) en haute dimension. La Naive de Bayes prend moins d'effort de régularisation contrairement à un modèle comme la régression logistique qui est plus prône d'être affecté par la haute dimensionnalité sans critère de régularisation bien défini.
\subsubsection{Entraînement}
Lors de la phase d'entraînement, estime le postérieur de Bayes par le jeu de donnés d'entraînement. Posons, $C \in \{0, 1\}$ étant la variable réponse $0$ ou $1$, et $x_i \in X_{train}$. Une hypothèse cruciale à cet algorithme est l'indépendance des attributs tel que $\forall x_i \in X \quad \forall j, k \in x_i$ avec $j$ et $k$ des composantes du vecteur $x_i$, nous avons:
$$\mathbb{P}[X = x_{i, j}, X = x_{i, k}] = \mathbb{P}[X = x_{i, j}] \mathbb{P}[X = x_{i, k}]$$ Nous avons par le théorème de Bayes:\\
$$\mathbb{P}[C | X=x_i] = \frac{\mathbb{P}[X = x_{i}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]} =  \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
Le critère de classification est le suivant:
$$\argmax_{C_i \in C, x_i \in X} \quad P[C_i | X = x_i] = \argmax_{C_i \in C, x_i \in X} \frac{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]}{\mathbb{P}[X = x_i]}$$
De façon équivalente, nous pouvons écrire le critère en fonction de la vraisemblance (le numérateur de l'expression ci-haut), car maximiser la probabilité, est équivalent à maximiser la vraisemblance, qui est aussi équivalent à maximiser la log vraisemblance, car les deux sont des fonctions croissantes. Voici le critère réécrit en fonction de la log vraisemblance:
$$\argmax_{C_i \in C, x_i \in X} \log{\prod_{j = 1}^{d}\mathbb{P}[X = x_{i,j}| C]\mathbb{P}[C]} = \argmax_{C_i \in C, x_i \in X} \sum_{j = 1}^{d} \log{\mathbb{P}[X = x_{i,j}| C]} + \log{\mathbb{P}[C]}$$
En pratique, il est important de voir que toutes les probabilités dans les expressions ci-haut sont calculables par le biais de fréquences observées dans le jeu de données d'entraînement. $\mathbb{P}[C]$ est la probabilité à priori d'être dans la classe 0 ou 1, donc $\mathbb{P}[C] = \frac{\text{Nombre d'observations dans la classe $i$}}{\text{Nombre d'observations dans $X_{train}$}}$. Similairement, $\mathbb{P}[X = x_{i, j}| C]$ est la probabilité que $x_{i, j}$, sachant la classe $C$, qui représente $\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$}}{\text{Nombre total de mots dans la classe $C_i$}}$. Cela dit, il est important de noter qu'il faut lisser cette probabilité afin d'éviter d'annuler la vraisemblance qui se produit lorsque $\mathbb{P}[X = x_{i, j}| C] = 0$, qui survient lorsque un mot $j$ ne se trouve pas dans la classe $C_i$. Posons $\alpha \in \mathbb{R}^{+}$ et $\alpha > 0$ l'hyperparamètre de lissage. Nous calculons dorénavant la vraisemblance, $$\mathbb{P}[X = x_{i, j}| C] = \frac{\text{Nombre de $j$ dans la classe $C_i$ + $\alpha$}}{\text{Nombre total de mots dans la classe $C_i$ + $\alpha \times$ nombre de mots dans le vocabulaire}}$$. Une grande valeur pour $\alpha$ applique un fort lissage sur toute les probabilités, alors qu'une faible valeur de $\alpha$ produit un lissage plus dur. Nous pouvons trouver le $\alpha$ optimal par validation croisée. 
\subsection{Validation croisée}
Nous implémentons une validation croisée (k-fold cross validation) avec $k = 7$ pour trouver l'hyperparamètre $\alpha$ optimal. Nous avons établi un espace hyperparamétrique de valeur espacée uniformément, allant de $0.4$ à $1.05$ avec des sauts de $0.05$. Du au fait que l'espace est relativement restreint, cet espace est visité séquentiellement par une recherche en grille (grid-search). 


\section{Algorithmes d'apprentissage utilisés}
Nous avons effectué multiples expériences avec multiples algorithmes d'apprentissage différents. Les algorithmes qui ont été les plus efficaces sont ceux présentés ci-après.
\subsection{Classifieur Bayes naïf}

\subsection{Classifieur XGBoost}

\subsection{Classifieur régression ridge}

\subsection{Classifieur d'apprentissage par ensembles}

\subsection{Autres}
Ces prochains algorithmes de classification ont aussi été essayés sans que leurs résultats soient assez concluants.
\subsubsection{Classifieur par régression logistique}
\subsubsection{Classifieur SVM}
\subsubsection{Classifieur par fôrets aléatoires}
\subsubsection{Classifieur par réseaux de neurones}

\section{Méthodologie}
\subsection{Répartition pour l'entrainement et la validation}
\subsection{Ajustement des hyperparamètres}
Gridsearch (linear et log)
randomsearch
k-fold

\subsection{Astuces d'optimisation}

\section{Résultats}
\subsection{Étape 1}
\subsection{Étape 2}

\section{Discussion}
\subsection{Avantages et inconvénients}
\subsection{Améliorations futures}

\section{Références}
les sites desquels nous nous sommes inspirés

\section{Annexes}
graphiques
\end{document}