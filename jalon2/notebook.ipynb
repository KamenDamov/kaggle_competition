{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aMTO_mUtK7Xy"
   },
   "source": [
    "# Importer les librairies, et fichiers .py auxiliaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:26:03.006958Z",
     "start_time": "2024-11-12T19:26:01.529992Z"
    },
    "id": "aDxdetLcJCvp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from preprocess_data import *\n",
    "from complement_naive_bayes import train_cnb_with_tfidf, train_cnb\n",
    "from ensemble_learning import train_ensemble\n",
    "from xgboost_classifier import train_xgboost\n",
    "from logistic_regression import *\n",
    "from svc_classifier import *\n",
    "from sgd_classifier import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from save_output import save_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:26:03.693064Z",
     "start_time": "2024-11-12T19:26:03.008726Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "X_train, y_train, X_test = data_preprocess.train, data_preprocess.label_train, data_preprocess.test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jalon 2)\n",
    "# Complement Naive Bayes\n",
    "K-Fold validation croisée stratifiée ($k = 5)$:\n",
    "- α: Lissage du postérieur de Bayes\n",
    " \n",
    "Prétraitement: TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:30:49.687633Z",
     "start_time": "2024-11-12T19:29:19.129640Z"
    }
   },
   "outputs": [],
   "source": [
    "complement_naive_bayes, tfidf_transformer = train_cnb_with_tfidf(X_train, y_train)\n",
    "tfidf_test = tfidf_transformer.transform(X_test)\n",
    "predictions = complement_naive_bayes.predict(tfidf_test)\n",
    "save_output(predictions, \"cnb\", \"random_search_10_iter\", \"tfidf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement: TFIDF, réduction par arbre, stopwords retirés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:31:32.164742Z",
     "start_time": "2024-11-12T19:31:14.471829Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "X_train, sorted_indices_features = tree_based_dimensionality_reduction(data_preprocess.train, data_preprocess.label_train)\n",
    "X_test = data_preprocess.test[:, sorted_indices_features]\n",
    "complement_naive_bayes, tfidf_transformer = train_cnb_with_tfidf(X_train, data_preprocess.label_train)\n",
    "tfidf_test = tfidf_transformer.transform(X_test)\n",
    "predictions = complement_naive_bayes.predict(tfidf_test)\n",
    "save_output(predictions, \"cnb\", \"random_search_10_iter\", \"tree_reduction_stopwords_tfidf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement: TFIDF, réduction par arbre, stopwords retirés, suréchantillonnage SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:31:46.255189Z",
     "start_time": "2024-11-12T19:31:32.170561Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "X_train, sorted_indices_features = tree_based_dimensionality_reduction(data_preprocess.train, data_preprocess.label_train)\n",
    "X_test = data_preprocess.test[:, sorted_indices_features]\n",
    "X_train, y_train = smote_oversampling(X_train, data_preprocess.label_train)\n",
    "\n",
    "complement_naive_bayes = train_cnb(X_train, y_train)\n",
    "tfidf_test = tfidf_transformer.transform(X_test)\n",
    "predictions = complement_naive_bayes.predict(tfidf_test)\n",
    "\n",
    "save_output(predictions, \"cnb\", \"random_search_10_iter\", \"tree_reduction_stopwords_tfidf_smote\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:31:51.918601Z",
     "start_time": "2024-11-12T19:31:46.271900Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "complement_naive_bayes = train_cnb(X_train_undersampled, y_train_undersampled)\n",
    "predictions = complement_naive_bayes.predict(data_preprocess.test)\n",
    "\n",
    "save_output(predictions, \"cnb\", \"random_search_10_iter\", \"stopwords_cum-sum_undersampled\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC\n",
    "Grid Search, K-Fold validation croisée stratifiée ($k = 5)$, avec hyperparamètres:\n",
    "- $\\gamma$ : Coefficient du Noyau RBF\n",
    "- $C$ : Terme de régularization pour la pénalité euclidienne.\n",
    "\n",
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:35:13.495015Z",
     "start_time": "2024-11-12T19:31:51.924711Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "best_params_, best_score_ = train_svc(X_train_undersampled, y_train_undersampled)\n",
    "print(best_params_, best_score_)\n",
    "svc = SVC(kernel='rbf', C=best_params_['C'], gamma=best_params_['gamma'])\n",
    "svc.fit(X_train_undersampled, y_train_undersampled)\n",
    "y_pred = svc.predict(data_preprocess.test)\n",
    "params = f\"C={best_params_['C']}, gamma={best_params_['gamma']}\"\n",
    "save_output(y_pred, \"svm\", params, \"stopwords_cum-sum_undersampled\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier\n",
    "Random Search, K-Fold validation croisée stratifiée ($k = 5)$, avec hyperparamètres:\n",
    "- loss: ModifiedHuber \n",
    "- penalty: ElaticNet \n",
    "- l1_ratio: Porportion de la perte d'ElasticNet qui est l1.\n",
    "- $\\alpha$: Poids attribué au terme de régularisation. Une plus grande valeur favorise que certains coefficients soient annulés (par l1) ou fortement adoucis (par l2) quand la pénalité est ElaticNet. \n",
    "\n",
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:36:30.415177Z",
     "start_time": "2024-11-12T19:35:13.503567Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "best_sgd = train_sgd(X_train_undersampled, y_train_undersampled)\n",
    "predictions_voter = best_sgd.predict(data_preprocess.test)\n",
    "save_output(predictions_voter, \"sgd\", \"random_search_10_iter\", \"stopwords_undersampling_cumulative_sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression Logistique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search, K-Fold validation croisée stratifiée ($k = 5)$, avec hyperparamètres: \n",
    "- penalty: Terme de régularisation l1 \n",
    "- solver: Porportion de la perte d'ElasticNet qui est l1.\n",
    "- $C$: Poids de la régularisation\n",
    "\n",
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "best_params_, best_score_ = train_logreg(X_train_undersampled, y_train_undersampled)\n",
    "print(best_params_, best_score_)\n",
    "svc = LogisticRegression(solver='liblinear', penalty = 'l1', C=best_params_['C'])\n",
    "svc.fit(X_train_undersampled, y_train_undersampled)\n",
    "y_pred = svc.predict(data_preprocess.test)\n",
    "params = f\"C={best_params_['C']}, gamma={best_params_['gamma']}\"\n",
    "save_output(y_pred, \"logreg\", params, \"stopwords_cum-sum_undersampled\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "Random Search, K-Fold validation croisée stratifiée ($k = 5)$, avec hyperparamètres:\n",
    "- Learning rate: Taille du pas lors de la descente de critère sur la perte (gain de subdivision) \n",
    "- Nombre d'estimateurs (n_estimators): Nombre d'arbres de décision\n",
    "- Profondeur maximale (max_depth): Profondeur maximale de chaque arbre (nombre de branchements max)\n",
    "- sous-échantillon (subsample):  Proportion de données utilisées pour produire chaque arbre.\n",
    "\n",
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:40:13.806597Z",
     "start_time": "2024-11-12T19:36:30.438848Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "xgboost_classifier = train_xgboost(X_train_undersampled, y_train_undersampled)\n",
    "predictions = xgboost_classifier.predict(data_preprocess.test)\n",
    "save_output(predictions, \"xgboost\", \"random_search_10_iter\", \"stopwords_undersampling_cumulative_sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage par ensembles: CNB, XGBoost, Logistic Regression\n",
    "\n",
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T20:02:59.490983Z",
     "start_time": "2024-11-12T19:40:13.810475Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "model_names = ['ComplementNB', 'XGBoost', 'LogisticRegression']\n",
    "# Validation croisée du voteur\n",
    "for train_index, test_index in kf.split(X_train_undersampled, y_train_undersampled):\n",
    "    # Split données\n",
    "    X_train, X_test = X_train_undersampled[train_index], X_train_undersampled[test_index]\n",
    "    y_train, y_test = y_train_undersampled[train_index], y_train_undersampled[test_index]\n",
    "    best_ensemble_model = train_ensemble(X_train, y_train, model_names)    \n",
    "    y_pred = best_ensemble_model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "print(\"Score F1 de validation du voteur: \", mean_score)\n",
    "\n",
    "best_ensemble_model = train_ensemble(X_train_undersampled, y_train_undersampled, model_names)\n",
    "predictions_voter = best_ensemble_model.predict(data_preprocess.test)\n",
    "save_output(predictions_voter, \"ensemble_cnb_xgboost_logreg\", \"random_search_10_iter\", \"stopwords_undersampling_cumulative_sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement: Retirer stopwords, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T21:00:21.107836Z",
     "start_time": "2024-11-12T20:02:59.534340Z"
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "model_names = ['ComplementNB', 'XGBoost', 'LogisticRegression']\n",
    "# Validation croisée du voteur\n",
    "for train_index, test_index in kf.split(X_train_undersampled, y_train_undersampled):\n",
    "    # Split données\n",
    "    X_train, X_test = X_train_undersampled[train_index], X_train_undersampled[test_index]\n",
    "    y_train, y_test = y_train_undersampled[train_index], y_train_undersampled[test_index]\n",
    "    best_ensemble_model = train_ensemble(X_train, y_train, model_names)    \n",
    "    y_pred = best_ensemble_model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "print(\"Score F1 de validation du voteur: \", mean_score)\n",
    "\n",
    "best_ensemble_model = train_ensemble(X_train_undersampled, y_train_undersampled, model_names)\n",
    "predictions_voter = best_ensemble_model.predict(data_preprocess.test)\n",
    "save_output(predictions_voter, \"ensemble_cnb_xgboost_logreg\", \"random_search_10_iter\", \"stopwords_undersampling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage par ensembles: Complement Naive Bayes, XGBoost, SVC, SGD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement: Retirer stopwords, Réduction par somme cumulative, sous-échantillonage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-12T22:35:32.931424Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_preprocess = DataPreprocess()\n",
    "data_preprocess.remove_stopwords()\n",
    "data_preprocess.remove_cum_sum()\n",
    "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "model_names = ['ComplementNB', 'XGBoost', 'SVC', 'SGD']\n",
    "# Validation croisée du voteur\n",
    "for train_index, test_index in kf.split(X_train_undersampled, y_train_undersampled):\n",
    "    # Split données\n",
    "    X_train, X_test = X_train_undersampled[train_index], X_train_undersampled[test_index]\n",
    "    y_train, y_test = y_train_undersampled[train_index], y_train_undersampled[test_index]\n",
    "    best_ensemble_model = train_ensemble(X_train, y_train, model_names)    \n",
    "    y_pred = best_ensemble_model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "print(\"Score F1 de validation du voteur: \", mean_score)\n",
    "\n",
    "best_ensemble_model = train_ensemble(X_train_undersampled, y_train_undersampled, model_names)\n",
    "predictions_voter = best_ensemble_model.predict(data_preprocess.test)\n",
    "save_output(predictions_voter, \"ensemble_cnb_xgboost_svc_sgd\", \"random_search_10_iter\", \"stopwords_undersampling_cumulative_sum\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
