{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aMTO_mUtK7Xy"
      },
      "source": [
        "# Importer les librairies, et fichiers .py auxiliaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDxdetLcJCvp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from preprocess_data import *\n",
        "from bayes_classifier import BayesClassifier\n",
        "from complement_naive_bayes import train_cnb_with_tfidf, train_cnb\n",
        "from ensemble_learning import train_ensemble\n",
        "from xgboost_classifier import train_xgboost, train_xgboost_with_tfidf\n",
        "\n",
        "from save_output import save_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_preprocess = DataPreprocess()\n",
        "X_train, y_train, X_test = data_preprocess.train, data_preprocess.label_train, data_preprocess.test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sHdKFfvqLWn-"
      },
      "source": [
        "# Jalon 1) Naive de Bayes vanille\n",
        "K-Fold Validation croisée ($k = 7)$:\n",
        "- α: Lissage du postérieur de Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "iD7VY-FYJE02",
        "outputId": "a6dae65d-3d11-40f9-c380-6c51f2aef693"
      },
      "outputs": [],
      "source": [
        "bayes_classifier = BayesClassifier()\n",
        "best_lps, best_f1 = bayes_classifier.hyperparameter_tuning(X_train, y_train)\n",
        "print(best_lps, best_f1)\n",
        "tuned_bayes_classifier = BayesClassifier()\n",
        "tuned_bayes_classifier.fit(X_train, y_train, best_lps)\n",
        "predictions = np.array([tuned_bayes_classifier.predict(x_i) for x_i in X_test])\n",
        "save_output(predictions, \"bayes\", best_lps, \"stopwords\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jalon 2)\n",
        "# Naive Bayes Complement\n",
        "K-Fold validation croisée stratifiée ($k = 5)$:\n",
        "- α: Lissage du postérieur de Bayes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement: TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complement_naive_bayes, tfidf_transformer = train_cnb_with_tfidf(X_train, y_train)\n",
        "tfidf_test = tfidf_transformer.transform(X_test)\n",
        "predictions = complement_naive_bayes.predict(tfidf_test)\n",
        "save_output(predictions, \"cnb\", \"random_search_50_iter\", \"tfidf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement: Sans prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complement_naive_bayes = train_cnb(X_train, y_train)\n",
        "predictions = complement_naive_bayes.predict(X_test)\n",
        "save_output(predictions, \"cnb\", \"random_search_50_iter\", \"no_preprocessing\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement: TFIDF, réduction par arbre, stopwords retirés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_preprocess = DataPreprocess()\n",
        "data_preprocess.remove_stopwords()\n",
        "X_train, sorted_indeces_features = tree_based_dimensionality_reduction(data_preprocess.train, data_preprocess.label_train)\n",
        "X_test = data_preprocess.test[:, sorted_indeces_features]\n",
        "complement_naive_bayes = train_cnb(X_train, data_preprocess.label_train)\n",
        "predictions = complement_naive_bayes.predict(data_preprocess.test)\n",
        "save_output(predictions, \"cnb\", \"random_search_50_iter\", \"tree_reduction_stopwords_tfidf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVC"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SGDClassifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost\n",
        "Random Search, K-Fold validation croisée stratifiée ($k = 5)$, avec hyperparamètres:\n",
        "- Learning rate: Taille du pas lors de la descente de critère sur la perte (gain de subdivision) \n",
        "- Nombre d'estimateurs (n_estimators): Nombre d'arbres de décision\n",
        "- Profondeur maximale (max_depth): Profondeur maximale de chaque arbre (nombre de branchements max)\n",
        "- sous-échantillon (subsample):  Proportion de données utilisées pour produire chaque arbre."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement: Sur-échantillonnage bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_preprocess = DataPreprocess()\n",
        "data_preprocess.remove_stopwords()\n",
        "x_train_oversampled, y_train_oversampled = boostrap_oversampling(data_preprocess.train, data_preprocess.label_train)\n",
        "xgboost_classifier, tfidf_transformer = train_xgboost(x_train_oversampled, y_train_oversampled)\n",
        "predictions = xgboost_classifier.predict(data_preprocess.test)\n",
        "save_output(predictions, \"xgboost\", \"random_search_15_iter\", \"stopwords_bootstrap\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apprentissage par Ensemble"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensmeble: CNB, XGBoost, Logistic Regression sans stopwords, sous-échantillonage"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement : sans stopwords, sous-échantillonage, réduction de dimension par somme cumulative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_preprocess = DataPreprocess()\n",
        "data_preprocess.remove_stopwords()\n",
        "indeces_to_remove =remove_cum_sum(data_preprocess.train, 0.95)\n",
        "data_preprocess.train = np.delete(data_preprocess.train, indeces_to_remove, axis=1)\n",
        "data_preprocess.test = np.delete(data_preprocess.test, indeces_to_remove, axis=1)\n",
        "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
        "model_names = ['ComplementNB', 'XGBoost', 'LogisticRegression']\n",
        "best_ensemble_model = train_ensemble(X_train_undersampled, y_train_undersampled, model_names)\n",
        "predictions_voter = best_ensemble_model.predict(data_preprocess.test)\n",
        "save_output(predictions_voter, \"ensemble_cnb_xgboost_logreg\", \"random_search_15_iter\", \"stopwords_undersampling_cumulative_sum\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prétraitement : sans stopwords, sous-échantillonage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_preprocess = DataPreprocess()\n",
        "data_preprocess.remove_stopwords()\n",
        "X_train_undersampled, y_train_undersampled = random_undersampling(data_preprocess.train, data_preprocess.label_train)\n",
        "model_names = ['ComplementNB', 'XGBoost', 'LogisticRegression']\n",
        "best_ensemble_model = train_ensemble(X_train_undersampled, y_train_undersampled, model_names)\n",
        "predictions_voter = best_ensemble_model.predict(data_preprocess.test)\n",
        "save_output(predictions_voter, \"ensemble_cnb_xgboost_logreg\", \"random_search_15_iter\", \"stopwords_undersampling_cumulative_sum\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
